{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import preprocessing\n",
    "import os\n",
    "import fnmatch\n",
    "from hmmlearn.hmm import GMMHMM\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(id,n_mfcc, dataset_dir):\n",
    "    file_paths = []\n",
    "    data = []\n",
    "    \n",
    "    file_pattern = f\"*{id}.wav\"\n",
    "    for root, _, files in os.walk(os.path.join(dataset_dir)):\n",
    "        for file in files:\n",
    "            if fnmatch.fnmatch(file.lower(), file_pattern.lower()):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "    \n",
    "    class_data = [preprocessing.get_mfcc(file_path, n_mfcc) for file_path in file_paths]\n",
    "    data.extend(class_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, n_state, n_mix): \n",
    "    start_prob = np.full(n_state, 0.0)\n",
    "    start_prob[0] = 1.0\n",
    "    trans_matrix = np.full((n_state, n_state), 0.0)\n",
    "    p = 0.5\n",
    "    np.fill_diagonal(trans_matrix, p)\n",
    "    np.fill_diagonal(trans_matrix[0:, 1:], 1 - p)\n",
    "    trans_matrix[-1, -1] = 1.0\n",
    "    \n",
    "    model = GMMHMM(\n",
    "                n_components=n_state,    \n",
    "                n_mix=n_mix,                         \n",
    "                verbose=False,\n",
    "                n_iter=300,\n",
    "                startprob_prior=start_prob,\n",
    "                transmat_prior=trans_matrix,\n",
    "                params='stmc',\n",
    "                init_params='mc',\n",
    "                random_state=42\n",
    "            )\n",
    "    model.fit(X=np.vstack(data),lengths=[x.shape[0] for x in data])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_log(log_dir, id, n_state, n_mix, n_mfcc, model):\n",
    "    os.makedirs(os.path.join(log_dir, id), exist_ok=True)\n",
    "    \n",
    "    log_file_path = os.path.join(log_dir, id, f\"{id}_{n_state}{n_mix}{n_mfcc}.log\")\n",
    "    with open(log_file_path, 'w') as log_file:\n",
    "        log_file.write(str(model.monitor_.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_path, id, n_state, n_mix, n_mfcc, model):\n",
    "    os.makedirs(os.path.join(model_path, id), exist_ok=True)\n",
    "    \n",
    "    name = f'{model_path}/{id}/{id}_{n_state}{n_mix}{n_mfcc}.pkl'\n",
    "    with open(name, 'wb') as file:\n",
    "        pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'data'\n",
    "model_path = 'models'\n",
    "train_dir = 'train'\n",
    "test_dir = 'test'\n",
    "log_dir = 'logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'01': {'name': 'chủ', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '02': {'name': 'về', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '03': {'name': 'vào', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '04': {'name': 'tải', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '05': {'name': 'đầu', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '06': {'name': 'cuối', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '07': {'name': 'kế', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '08': {'name': 'trước', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '09': {'name': 'dừng', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '10': {'name': 'ngưng', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '11': {'name': 'đọc', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '13': {'name': 'tiếp', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '12': {'name': 'lui', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '14': {'name': 'tới', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '15': {'name': 'tăng', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '16': {'name': 'to', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '17': {'name': 'giảm', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '18': {'name': 'nhỏ', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '19': {'name': 'lại', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '20': {'name': 'lặp', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '21': {'name': 'nhanh', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '22': {'name': 'chậm', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '23': {'name': 'lưu', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '24': {'name': 'xoá', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '25': {'name': 'huỷ', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '26': {'name': 'chạy', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '27': {'name': 'xong', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '28': {'name': 'đúng', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '29': {'name': 'sai', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '30': {'name': 'giúp', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '31': {'name': 'giờ', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '32': {'name': 'ngày', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '33': {'name': 'tươi', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '34': {'name': 'có', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '35': {'name': 'không', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '36': {'name': 'mục', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '37': {'name': 'bài', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '38': {'name': 'một', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '39': {'name': 'hai', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '40': {'name': 'ba', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '41': {'name': 'bốn', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '42': {'name': 'năm', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '43': {'name': 'sáu', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '44': {'name': 'bảy', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '45': {'name': 'tám', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '46': {'name': 'chín', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '47': {'name': 'a', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '48': {'name': 'e', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '49': {'name': 'i', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '50': {'name': 'o', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '51': {'name': 'u', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}, '52': {'name': 'thổ địa', 'n_state': [4, 5, 6, 7], 'n_mix': [2, 3, 4, 5], 'n_mfcc': [13]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not converging.  Current: -288578.352123678 is not greater than -288575.10878389614. Delta is -3.2433397818822414\n",
      "Model is not converging.  Current: -282510.4372055668 is not greater than -282460.4836738936. Delta is -49.95353167317808\n",
      "Model is not converging.  Current: -279017.01272482076 is not greater than -278997.231175748. Delta is -19.78154907276621\n",
      "Model is not converging.  Current: -280201.49008179124 is not greater than -280171.26910636737. Delta is -30.220975423872005\n",
      "Model is not converging.  Current: -279011.5831078965 is not greater than -279003.8527661493. Delta is -7.730341747170314\n",
      "Model is not converging.  Current: -277561.415655708 is not greater than -277554.38987978507. Delta is -7.0257759229280055\n",
      "Model is not converging.  Current: -268179.37991378433 is not greater than -268178.519911872. Delta is -0.8600019123405218\n",
      "Model is not converging.  Current: -270490.1447745572 is not greater than -270489.7714745192. Delta is -0.37330003798706457\n",
      "Model is not converging.  Current: -262577.81777411356 is not greater than -262572.6229804842. Delta is -5.194793629343621\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[171], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_state \u001b[38;5;129;01min\u001b[39;00m value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_state\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n_mix \u001b[38;5;129;01min\u001b[39;00m value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_mix\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 10\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_mfcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m         model \u001b[38;5;241m=\u001b[39m train(data\u001b[38;5;241m=\u001b[39mdata, n_state\u001b[38;5;241m=\u001b[39mn_state, n_mix\u001b[38;5;241m=\u001b[39mn_mix)\n\u001b[1;32m     12\u001b[0m         save_model(model_path\u001b[38;5;241m=\u001b[39mmodel_path, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mkey, n_state\u001b[38;5;241m=\u001b[39mn_state, n_mix\u001b[38;5;241m=\u001b[39mn_mix, n_mfcc\u001b[38;5;241m=\u001b[39mn_mfcc, model\u001b[38;5;241m=\u001b[39mmodel)\n",
      "Cell \u001b[0;32mIn[135], line 11\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(id, n_mfcc, dataset_dir)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fnmatch\u001b[38;5;241m.\u001b[39mfnmatch(file\u001b[38;5;241m.\u001b[39mlower(), file_pattern\u001b[38;5;241m.\u001b[39mlower()):\n\u001b[1;32m      9\u001b[0m             file_paths\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file))\n\u001b[0;32m---> 11\u001b[0m class_data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mfcc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m data\u001b[38;5;241m.\u001b[39mextend(class_data)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "Cell \u001b[0;32mIn[135], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fnmatch\u001b[38;5;241m.\u001b[39mfnmatch(file\u001b[38;5;241m.\u001b[39mlower(), file_pattern\u001b[38;5;241m.\u001b[39mlower()):\n\u001b[1;32m      9\u001b[0m             file_paths\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file))\n\u001b[0;32m---> 11\u001b[0m class_data \u001b[38;5;241m=\u001b[39m [\u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mfcc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m file_paths]\n\u001b[1;32m     12\u001b[0m data\u001b[38;5;241m.\u001b[39mextend(class_data)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/projects/101_HMMSpeechRecognition/preprocessing.py:12\u001b[0m, in \u001b[0;36mget_mfcc\u001b[0;34m(file_path, n_mfcc)\u001b[0m\n\u001b[1;32m     10\u001b[0m win_length \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mfloor(sr \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.025\u001b[39m)  \u001b[38;5;66;03m# 25ms frame\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# mfcc is 12 x T matrix\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m mfcc \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmfcc\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# subtract mean from mfcc --> normalize mfcc\u001b[39;00m\n\u001b[1;32m     14\u001b[0m mfcc \u001b[38;5;241m=\u001b[39m mfcc \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(mfcc, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.11/site-packages/librosa/feature/spectral.py:1989\u001b[0m, in \u001b[0;36mmfcc\u001b[0;34m(y, sr, S, n_mfcc, dct_type, norm, lifter, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mel-frequency cepstral coefficients (MFCCs)\u001b[39;00m\n\u001b[1;32m   1844\u001b[0m \n\u001b[1;32m   1845\u001b[0m \u001b[38;5;124;03m.. warning:: If multi-channel audio input ``y`` is provided, the MFCC\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;124;03m>>> fig.colorbar(img2, ax=[ax[1]])\u001b[39;00m\n\u001b[1;32m   1986\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m S \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1988\u001b[0m     \u001b[38;5;66;03m# multichannel behavior may be different due to relative noise floor differences between channels\u001b[39;00m\n\u001b[0;32m-> 1989\u001b[0m     S \u001b[38;5;241m=\u001b[39m power_to_db(\u001b[43mmelspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1991\u001b[0m M: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39mfftpack\u001b[38;5;241m.\u001b[39mdct(S, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mdct_type, norm\u001b[38;5;241m=\u001b[39mnorm)[\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :n_mfcc, :\n\u001b[1;32m   1993\u001b[0m ]\n\u001b[1;32m   1995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lifter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;66;03m# shape lifter for broadcasting\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.11/site-packages/librosa/feature/spectral.py:2145\u001b[0m, in \u001b[0;36mmelspectrogram\u001b[0;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[1;32m   2142\u001b[0m \u001b[38;5;66;03m# Build a Mel filter\u001b[39;00m\n\u001b[1;32m   2143\u001b[0m mel_basis \u001b[38;5;241m=\u001b[39m filters\u001b[38;5;241m.\u001b[39mmel(sr\u001b[38;5;241m=\u001b[39msr, n_fft\u001b[38;5;241m=\u001b[39mn_fft, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 2145\u001b[0m melspec: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m...ft,mf->...mt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_basis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m melspec\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.11/site-packages/numpy/core/einsumfunc.py:1419\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1416\u001b[0m     right_pos\u001b[38;5;241m.\u001b[39mappend(input_right\u001b[38;5;241m.\u001b[39mfind(s))\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;66;03m# Contract!\u001b[39;00m\n\u001b[0;32m-> 1419\u001b[0m new_view \u001b[38;5;241m=\u001b[39m \u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtmp_operands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mleft_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mright_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;66;03m# Build a new view if needed\u001b[39;00m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (tensor_result \u001b[38;5;241m!=\u001b[39m results_index) \u001b[38;5;129;01mor\u001b[39;00m handle_out:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.11/site-packages/numpy/core/numeric.py:1121\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(a, b, axes)\u001b[0m\n\u001b[1;32m   1119\u001b[0m at \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mtranspose(newaxes_a)\u001b[38;5;241m.\u001b[39mreshape(newshape_a)\n\u001b[1;32m   1120\u001b[0m bt \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mtranspose(newaxes_b)\u001b[38;5;241m.\u001b[39mreshape(newshape_b)\n\u001b[0;32m-> 1121\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mreshape(olda \u001b[38;5;241m+\u001b[39m oldb)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(\"config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "print(config)\n",
    "\n",
    "for key, value in config.items():\n",
    "    for n_mfcc in value['n_mfcc']:\n",
    "        for n_state in value['n_state']:\n",
    "            for n_mix in value['n_mix']:\n",
    "                data = prepare_data(id=key, n_mfcc=n_mfcc, dataset_dir=os.path.join(dataset_path, train_dir))\n",
    "                model = train(data=data, n_state=n_state, n_mix=n_mix)\n",
    "                save_model(model_path=model_path, id=key, n_state=n_state, n_mix=n_mix, n_mfcc=n_mfcc, model=model)\n",
    "                save_log(log_dir=log_dir, id=key, n_state=n_state, n_mix=n_mix, n_mfcc=n_mfcc, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1/5 for ID: 01\n",
      "Validation log-likelihood for fold 1: -11570.48\n",
      "Training fold 2/5 for ID: 01\n",
      "Validation log-likelihood for fold 2: -16746.24\n",
      "Training fold 3/5 for ID: 01\n",
      "Validation log-likelihood for fold 3: -13467.22\n",
      "Training fold 4/5 for ID: 01\n",
      "Validation log-likelihood for fold 4: -14838.57\n",
      "Training fold 5/5 for ID: 01\n",
      "Validation log-likelihood for fold 5: -16570.21\n",
      "Training fold 1/5 for ID: 02\n",
      "Validation log-likelihood for fold 1: -12346.66\n",
      "Training fold 2/5 for ID: 02\n",
      "Validation log-likelihood for fold 2: -14366.81\n",
      "Training fold 3/5 for ID: 02\n",
      "Validation log-likelihood for fold 3: -14560.66\n",
      "Training fold 4/5 for ID: 02\n",
      "Validation log-likelihood for fold 4: -15995.41\n",
      "Training fold 5/5 for ID: 02\n",
      "Validation log-likelihood for fold 5: -13218.62\n",
      "Training fold 1/5 for ID: 03\n",
      "Validation log-likelihood for fold 1: -12455.83\n",
      "Training fold 2/5 for ID: 03\n",
      "Validation log-likelihood for fold 2: -14404.43\n",
      "Training fold 3/5 for ID: 03\n",
      "Validation log-likelihood for fold 3: -15989.22\n",
      "Training fold 4/5 for ID: 03\n",
      "Validation log-likelihood for fold 4: -16050.36\n",
      "Training fold 5/5 for ID: 03\n",
      "Validation log-likelihood for fold 5: -14278.73\n",
      "Training fold 1/5 for ID: 04\n",
      "Validation log-likelihood for fold 1: -12711.93\n",
      "Training fold 2/5 for ID: 04\n",
      "Validation log-likelihood for fold 2: -14431.98\n",
      "Training fold 3/5 for ID: 04\n",
      "Validation log-likelihood for fold 3: -14833.19\n",
      "Training fold 4/5 for ID: 04\n",
      "Validation log-likelihood for fold 4: -15441.20\n",
      "Training fold 5/5 for ID: 04\n",
      "Validation log-likelihood for fold 5: -13868.51\n",
      "Training fold 1/5 for ID: 05\n",
      "Validation log-likelihood for fold 1: -12341.45\n",
      "Training fold 2/5 for ID: 05\n",
      "Validation log-likelihood for fold 2: -14793.56\n",
      "Training fold 3/5 for ID: 05\n",
      "Validation log-likelihood for fold 3: -15459.32\n",
      "Training fold 4/5 for ID: 05\n",
      "Validation log-likelihood for fold 4: -15789.47\n",
      "Training fold 5/5 for ID: 05\n",
      "Validation log-likelihood for fold 5: -13878.10\n",
      "Training fold 1/5 for ID: 06\n",
      "Validation log-likelihood for fold 1: -12751.19\n",
      "Training fold 2/5 for ID: 06\n",
      "Validation log-likelihood for fold 2: -15035.98\n",
      "Training fold 3/5 for ID: 06\n",
      "Validation log-likelihood for fold 3: -15278.73\n",
      "Training fold 4/5 for ID: 06\n",
      "Validation log-likelihood for fold 4: -15572.39\n",
      "Training fold 5/5 for ID: 06\n",
      "Validation log-likelihood for fold 5: -13120.43\n",
      "Training fold 1/5 for ID: 07\n",
      "Validation log-likelihood for fold 1: -11938.78\n",
      "Training fold 2/5 for ID: 07\n",
      "Validation log-likelihood for fold 2: -17094.48\n",
      "Training fold 3/5 for ID: 07\n",
      "Validation log-likelihood for fold 3: -12895.97\n",
      "Training fold 4/5 for ID: 07\n",
      "Validation log-likelihood for fold 4: -15130.32\n",
      "Training fold 5/5 for ID: 07\n",
      "Validation log-likelihood for fold 5: -14105.54\n",
      "Training fold 1/5 for ID: 08\n",
      "Validation log-likelihood for fold 1: -12648.98\n",
      "Training fold 2/5 for ID: 08\n",
      "Validation log-likelihood for fold 2: -16798.04\n",
      "Training fold 3/5 for ID: 08\n",
      "Validation log-likelihood for fold 3: -12852.35\n",
      "Training fold 4/5 for ID: 08\n",
      "Validation log-likelihood for fold 4: -13769.25\n",
      "Training fold 5/5 for ID: 08\n",
      "Validation log-likelihood for fold 5: -13427.21\n",
      "Training fold 1/5 for ID: 09\n",
      "Validation log-likelihood for fold 1: -12052.08\n",
      "Training fold 2/5 for ID: 09\n",
      "Validation log-likelihood for fold 2: -15160.64\n",
      "Training fold 3/5 for ID: 09\n",
      "Validation log-likelihood for fold 3: -14549.70\n",
      "Training fold 4/5 for ID: 09\n",
      "Validation log-likelihood for fold 4: -15493.56\n",
      "Training fold 5/5 for ID: 09\n",
      "Validation log-likelihood for fold 5: -12897.29\n",
      "Training fold 1/5 for ID: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not converging.  Current: -227154.37950394335 is not greater than -227150.7687193511. Delta is -3.6107845922524575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation log-likelihood for fold 1: -11669.16\n",
      "Training fold 2/5 for ID: 10\n",
      "Validation log-likelihood for fold 2: -14576.75\n",
      "Training fold 3/5 for ID: 10\n",
      "Validation log-likelihood for fold 3: -14940.46\n",
      "Training fold 4/5 for ID: 10\n",
      "Validation log-likelihood for fold 4: -14870.79\n",
      "Training fold 5/5 for ID: 10\n",
      "Validation log-likelihood for fold 5: -13266.92\n",
      "Training fold 1/5 for ID: 11\n",
      "Validation log-likelihood for fold 1: -12712.35\n",
      "Training fold 2/5 for ID: 11\n",
      "Validation log-likelihood for fold 2: -14432.14\n",
      "Training fold 3/5 for ID: 11\n",
      "Validation log-likelihood for fold 3: -15002.88\n",
      "Training fold 4/5 for ID: 11\n",
      "Validation log-likelihood for fold 4: -14948.89\n",
      "Training fold 5/5 for ID: 11\n",
      "Validation log-likelihood for fold 5: -12751.61\n",
      "Training fold 1/5 for ID: 13\n",
      "Validation log-likelihood for fold 1: -13072.95\n",
      "Training fold 2/5 for ID: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Degenerate mixture covariance\n",
      "Degenerate mixture covariance\n",
      "Degenerate mixture covariance\n",
      "Degenerate mixture covariance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation log-likelihood for fold 2: -15592.90\n",
      "Training fold 3/5 for ID: 13\n",
      "Validation log-likelihood for fold 3: -14734.90\n",
      "Training fold 4/5 for ID: 13\n",
      "Validation log-likelihood for fold 4: -15989.11\n",
      "Training fold 5/5 for ID: 13\n",
      "Validation log-likelihood for fold 5: -13847.02\n",
      "Training fold 1/5 for ID: 12\n",
      "Validation log-likelihood for fold 1: -12221.74\n",
      "Training fold 2/5 for ID: 12\n",
      "Validation log-likelihood for fold 2: -13684.01\n",
      "Training fold 3/5 for ID: 12\n",
      "Validation log-likelihood for fold 3: -14395.10\n",
      "Training fold 4/5 for ID: 12\n",
      "Validation log-likelihood for fold 4: -14127.53\n",
      "Training fold 5/5 for ID: 12\n",
      "Validation log-likelihood for fold 5: -12623.55\n",
      "Training fold 1/5 for ID: 14\n",
      "Validation log-likelihood for fold 1: -13322.39\n",
      "Training fold 2/5 for ID: 14\n",
      "Validation log-likelihood for fold 2: -15865.20\n",
      "Training fold 3/5 for ID: 14\n",
      "Validation log-likelihood for fold 3: -15336.87\n",
      "Training fold 4/5 for ID: 14\n",
      "Validation log-likelihood for fold 4: -15993.06\n",
      "Training fold 5/5 for ID: 14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[166], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m val_data \u001b[38;5;241m=\u001b[39m [data[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m val_index]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Train the model on training data\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_state\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_mix\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Optionally, evaluate the model on validation data (if evaluation metric is needed)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# For example, log-likelihood can be used as an evaluation metric\u001b[39;00m\n\u001b[1;32m     28\u001b[0m log_likelihoods \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mscore(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m val_data]\n",
      "Cell \u001b[0;32mIn[158], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data, n_state, n_mix)\u001b[0m\n\u001b[1;32m      8\u001b[0m trans_matrix[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m GMMHMM(\n\u001b[1;32m     11\u001b[0m             n_components\u001b[38;5;241m=\u001b[39mn_state,    \n\u001b[1;32m     12\u001b[0m             n_mix\u001b[38;5;241m=\u001b[39mn_mix,                         \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m             random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     20\u001b[0m         )\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.11/site-packages/hmmlearn/base.py:480\u001b[0m, in \u001b[0;36m_AbstractHMM.fit\u001b[0;34m(self, X, lengths)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lengths \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m--> 480\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check()\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitor_\u001b[38;5;241m.\u001b[39m_reset()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.11/site-packages/hmmlearn/hmm.py:550\u001b[0m, in \u001b[0;36mGMMHMM._init\u001b[0;34m(self, X, lengths)\u001b[0m\n\u001b[1;32m    546\u001b[0m main_kmeans \u001b[38;5;241m=\u001b[39m cluster\u001b[38;5;241m.\u001b[39mKMeans(n_clusters\u001b[38;5;241m=\u001b[39mnc,\n\u001b[1;32m    547\u001b[0m                              random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state,\n\u001b[1;32m    548\u001b[0m                              n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# sklearn >=1.2 compat.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# covariance matrix\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mmain_kmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m main_centroid \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(main_kmeans\u001b[38;5;241m.\u001b[39mcluster_centers_, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    552\u001b[0m means \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1071\u001b[0m, in \u001b[0;36m_BaseKMeans.fit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \n\u001b[1;32m   1051\u001b[0m \u001b[38;5;124;03m    Convenience method; equivalent to calling fit(X) followed by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m        Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1071\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1525\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1521\u001b[0m best_inertia, best_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_init):\n\u001b[1;32m   1524\u001b[0m     \u001b[38;5;66;03m# Initialize centers\u001b[39;00m\n\u001b[0;32m-> 1525\u001b[0m     centers_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_centroids\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_squared_norms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1532\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m   1533\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1021\u001b[0m, in \u001b[0;36m_BaseKMeans._init_centroids\u001b[0;34m(self, X, x_squared_norms, init, random_state, sample_weight, init_size, n_centroids)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight[init_indices]\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk-means++\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1021\u001b[0m     centers, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_kmeans_plusplus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_squared_norms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m init \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1029\u001b[0m     seeds \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mchoice(\n\u001b[1;32m   1030\u001b[0m         n_samples,\n\u001b[1;32m   1031\u001b[0m         size\u001b[38;5;241m=\u001b[39mn_clusters,\n\u001b[1;32m   1032\u001b[0m         replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1033\u001b[0m         p\u001b[38;5;241m=\u001b[39msample_weight \u001b[38;5;241m/\u001b[39m sample_weight\u001b[38;5;241m.\u001b[39msum(),\n\u001b[1;32m   1034\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:238\u001b[0m, in \u001b[0;36m_kmeans_plusplus\u001b[0;34m(X, n_clusters, x_squared_norms, sample_weight, random_state, n_local_trials)\u001b[0m\n\u001b[1;32m    235\u001b[0m indices[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m center_id\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Initialize list of closest distances and calculate current potential\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m closest_dist_sq \u001b[38;5;241m=\u001b[39m \u001b[43m_euclidean_distances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_norm_squared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    240\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m current_pot \u001b[38;5;241m=\u001b[39m closest_dist_sq \u001b[38;5;241m@\u001b[39m sample_weight\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Pick the remaining n_clusters-1 points\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:379\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[0;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[1;32m    374\u001b[0m         YY \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32 \u001b[38;5;129;01mor\u001b[39;00m Y\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;66;03m# To minimize precision issues with float32, we compute the distance\u001b[39;00m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;66;03m# matrix on chunks of X and Y upcast to float64\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[43m_euclidean_distances_upcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;66;03m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m safe_sparse_dot(X, Y\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:591\u001b[0m, in \u001b[0;36m_euclidean_distances_upcast\u001b[0;34m(X, XX, Y, YY, batch_size)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    589\u001b[0m     YY_chunk \u001b[38;5;241m=\u001b[39m YY[:, y_slice]\n\u001b[0;32m--> 591\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_chunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m d \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m XX_chunk\n\u001b[1;32m    593\u001b[0m d \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m YY_chunk\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.11/site-packages/sklearn/utils/extmath.py:211\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     ret \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m b\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 211\u001b[0m     \u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m ):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.11/site-packages/scipy/sparse/_base.py:1335\u001b[0m, in \u001b[0;36missparse\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A namespace class to separate sparray from spmatrix\"\"\"\u001b[39;00m\n\u001b[1;32m   1332\u001b[0m sparray\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m _spbase\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[0;32m-> 1335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21missparse\u001b[39m(x):\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Is `x` of a sparse array or sparse matrix type?\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _spbase)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# import numpy as np\n",
    "\n",
    "# # Define number of folds\n",
    "# k_folds = 5\n",
    "\n",
    "# # Main loop for training models\n",
    "# for key, value in config.items():\n",
    "#     # Prepare data and labels\n",
    "#     data = prepare_data(id=key, n_mfcc=value['n_mfcc'], dataset_dir=os.path.join(dataset_path, train_dir))\n",
    "    \n",
    "#     # Initialize K-Fold\n",
    "#     kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "#     fold = 1\n",
    "    \n",
    "#     for train_index, val_index in kf.split(data):\n",
    "#         print(f\"Training fold {fold}/{k_folds} for ID: {key}\")\n",
    "        \n",
    "#         # Split data into training and validation sets\n",
    "#         train_data = [data[i] for i in train_index]\n",
    "#         val_data = [data[i] for i in val_index]\n",
    "        \n",
    "#         # Train the model on training data\n",
    "#         model = train(data=train_data, n_state=value['n_state'], n_mix=value['n_mix'])\n",
    "        \n",
    "#         # Optionally, evaluate the model on validation data (if evaluation metric is needed)\n",
    "#         # For example, log-likelihood can be used as an evaluation metric\n",
    "#         log_likelihoods = [model.score(x) for x in val_data]\n",
    "#         print(f\"Validation log-likelihood for fold {fold}: {np.mean(log_likelihoods):.2f}\")\n",
    "        \n",
    "#         # Save model and logs for this fold\n",
    "#         save_model(f\"{model_path}/fold_{fold}\", key, model)\n",
    "#         save_log(f\"{log_dir}/fold_{fold}\", key, model)\n",
    "        \n",
    "#         fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, id):\n",
    "    with open(f'{model_path}/{id}.pkl', 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_models(model_path):\n",
    "    models = {}\n",
    "    for file in os.listdir(model_path):\n",
    "        if file.endswith(\".pkl\"):\n",
    "            id = file.split(\".\")[0]\n",
    "            model = load_model(model_path, id)\n",
    "            models[id] = model\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'01': {'name': 'chủ', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '02': {'name': 'về', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '03': {'name': 'vào', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '04': {'name': 'tải', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '05': {'name': 'đầu', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '06': {'name': 'cuối', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '07': {'name': 'kế', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '08': {'name': 'trước', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '09': {'name': 'dừng', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '10': {'name': 'ngưng', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '11': {'name': 'đọc', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '13': {'name': 'tiếp', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '12': {'name': 'lui', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '14': {'name': 'tới', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '15': {'name': 'tăng', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '16': {'name': 'to', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '17': {'name': 'giảm', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '18': {'name': 'nhỏ', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '19': {'name': 'lại', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '20': {'name': 'lặp', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '21': {'name': 'nhanh', 'n_state': 6, 'n_mix': 3, 'n_mfcc': 13}, '22': {'name': 'chậm', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '23': {'name': 'lưu', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '24': {'name': 'xoá', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '25': {'name': 'huỷ', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '26': {'name': 'chạy', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '27': {'name': 'xong', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '28': {'name': 'đúng', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '29': {'name': 'sai', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '30': {'name': 'giúp', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '31': {'name': 'giờ', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '32': {'name': 'ngày', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '33': {'name': 'tươi', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '34': {'name': 'có', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '35': {'name': 'không', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '36': {'name': 'mục', 'n_state': 3, 'n_mix': 3, 'n_mfcc': 13}, '37': {'name': 'bài', 'n_state': 2, 'n_mix': 3, 'n_mfcc': 13}, '38': {'name': 'một', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '39': {'name': 'hai', 'n_state': 3, 'n_mix': 4, 'n_mfcc': 13}, '40': {'name': 'ba', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '41': {'name': 'bốn', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '42': {'name': 'năm', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '43': {'name': 'sáu', 'n_state': 4, 'n_mix': 3, 'n_mfcc': 13}, '44': {'name': 'bảy', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '45': {'name': 'tám', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '46': {'name': 'chín', 'n_state': 5, 'n_mix': 3, 'n_mfcc': 13}, '47': {'name': 'a', 'n_state': 3, 'n_mix': 3, 'n_mfcc': 13}, '48': {'name': 'e', 'n_state': 3, 'n_mix': 3, 'n_mfcc': 13}, '49': {'name': 'i', 'n_state': 3, 'n_mix': 3, 'n_mfcc': 13}, '50': {'name': 'o', 'n_state': 3, 'n_mix': 3, 'n_mfcc': 13}, '51': {'name': 'u', 'n_state': 3, 'n_mix': 3, 'n_mfcc': 13}, '52': {'name': 'thổ địa', 'n_state': 8, 'n_mix': 3, 'n_mfcc': 13}}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_accuracy\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Assuming class_names is defined\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[161], line 9\u001b[0m, in \u001b[0;36mevaluation\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      7\u001b[0m all_y_true \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m all_y_pred \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 9\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mload_all_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m all_accuracy \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     12\u001b[0m index_to_name \u001b[38;5;241m=\u001b[39m {index: value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m index, (key, value) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(config\u001b[38;5;241m.\u001b[39mitems())}\n",
      "Cell \u001b[0;32mIn[143], line 3\u001b[0m, in \u001b[0;36mload_all_models\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_all_models\u001b[39m(model_path):\n\u001b[1;32m      2\u001b[0m     models \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(model_path):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models'"
     ]
    }
   ],
   "source": [
    "with open(\"config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "print(config)\n",
    "\n",
    "def evaluation(config):\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    models = load_all_models(model_path)\n",
    "    all_accuracy = {}\n",
    "    \n",
    "    index_to_name = {index: value['name'] for index, (key, value) in enumerate(config.items())}\n",
    "\n",
    "    for key, value in config.items():\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        test_data = prepare_data(key, value['n_mfcc'], os.path.join(dataset_path, test_dir))\n",
    "        test_labels = [value['name']] * len(test_data)  # Generate labels based on the length of test_data\n",
    "        \n",
    "        for data, label in zip(test_data, test_labels):\n",
    "            scores = [models[key].score(data) for key, value in config.items()]\n",
    "            pred = np.argmax(scores)\n",
    "            y_pred.append(index_to_name[pred])\n",
    "            y_true.append(label)\n",
    "        \n",
    "        accuracy = (np.array(y_true) == np.array(y_pred)).sum() / len(y_true)\n",
    "        \n",
    "        all_accuracy[value['name']] = accuracy\n",
    "        all_y_true.extend(y_true)\n",
    "        all_y_pred.extend(y_pred)\n",
    "\n",
    "    # Calculate confusion matrix using all accumulated true and predicted values\n",
    "    fig, ax = plt.subplots(figsize=(14,14))\n",
    "    ax.set(title='Confusion Matrix')\n",
    "    display_labels = [value['name'] for key, value in config.items()]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(all_y_true, all_y_pred), display_labels=display_labels)\n",
    "    disp.plot(ax=ax)\n",
    "\n",
    "    # Rotate the horizontal labels by 90 degrees\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    plt.show()\n",
    "    return all_accuracy\n",
    "\n",
    "# Assuming class_names is defined\n",
    "evaluation(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_n_mfcc = None\n",
    "best_number_of_states = None\n",
    "best_n_mix = None\n",
    "\n",
    "n_mfcc = 4\n",
    "# for n_mfcc in range(13, 20): \n",
    "for number_of_states in range(2, 7):\n",
    "    for n_mix in range(1, 6): \n",
    "        print(f\"n_mfcc: {n_mfcc}, number_of_states: {number_of_states}, n_mix: {n_mix}\")\n",
    "        try:\n",
    "            for class_name in class_names:\n",
    "                data, labels = prepare_train_data(class_names.index(class_name), class_name, n_mfcc)\n",
    "                model = train(class_name, data, number_of_states, n_mix, 300)\n",
    "                save_model(class_name, model)\n",
    "            all_accuracy = evaluation(class_names, n_mfcc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during training for n_mfcc: {n_mfcc}, number_of_states: {number_of_states}, n_mix: {n_mix}, error: {e}\")\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
